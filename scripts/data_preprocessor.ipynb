{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pdfplumber\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using pdfplumber.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans extracted text by removing headers, footers, special characters, and stopwords.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove multiple spaces and line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove headers and footers (simple heuristic based on page numbers)\n",
    "    text = re.sub(r'\\bPage\\s+\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    \n",
    "    # Remove special characters except for basic punctuation\n",
    "    text = re.sub(r'[^A-Za-z0-9.,;:?!()\\[\\] ]+', '', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans extracted text by removing headers, footers, special characters, and stopwords.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove multiple spaces and line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove headers and footers (simple heuristic based on page numbers)\n",
    "    text = re.sub(r'\\bPage\\s+\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    \n",
    "    # Remove special characters except for basic punctuation\n",
    "    text = re.sub(r'[^A-Za-z0-9.,;:?!()\\[\\] ]+', '', text)\n",
    "       \n",
    "    return text.strip()\n",
    "\n",
    "def get_company_files(companies: List[str], dir_path: str, file_extension: str = '.pdf') -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves file paths for each company.\n",
    "    \n",
    "    Args:\n",
    "        companies (List[str]): List of company identifiers.\n",
    "        dir_path (str): Directory path where the files are stored.\n",
    "        file_extension (str): File extension to look for.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Mapping from company to file path.\n",
    "    \"\"\"\n",
    "    company_files = {}\n",
    "    for company in companies:\n",
    "        # searching for filename containing the company identifier\n",
    "        for file in os.listdir(dir_path):\n",
    "            if file.lower().endswith(file_extension) and company.lower() in file.lower():\n",
    "                company_files[company] = os.path.join(dir_path, file)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: No file found for {company} in {dir_path}\")\n",
    "    return company_files\n",
    "\n",
    "def split_into_chunks(text: str, max_length: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into smaller chunks based on max_length.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "        max_length (int): Maximum number of characters per chunk.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of text chunks.\n",
    "    \"\"\"\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def process_data(config: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Processes PDF files and generates a JSONL file for fine-tuning.\n",
    "\n",
    "    Output data format\n",
    "\n",
    "    {\n",
    "        \"company\": \"CompanyA\",\n",
    "        \"input_text\": \"Company: CompanyA\\n[10-K Filing]\\n[Cleaned 10-K Text]\",\n",
    "        \"output_text\": \"Company: CompanyA\\n[Morningstar Report]\\n[Cleaned Report Text]\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Retrieve file mappings\n",
    "    tenk_files = get_company_files(config['companies'], config['10k_dir'])\n",
    "    report_files = get_company_files(config['companies'], config['reports_dir'])\n",
    "    \n",
    "    # Open output file\n",
    "    with open(config['output_file'], 'w', encoding='utf-8') as outfile:\n",
    "        # Iterate over companies\n",
    "        for company in tqdm(config['companies'], desc=\"Processing Companies\"):\n",
    "            tenk_path = tenk_files.get(company)\n",
    "            report_path = report_files.get(company)\n",
    "            \n",
    "            if not tenk_path or not report_path:\n",
    "                print(f\"Skipping {company} due to missing files.\")\n",
    "                continue\n",
    "            \n",
    "            # Extract and clean 10-K text\n",
    "            tenk_text = extract_text_from_pdf(tenk_path)\n",
    "            tenk_clean = clean_text(tenk_text)\n",
    "            \n",
    "            # Extract and clean Morningstar report text\n",
    "            report_text = extract_text_from_pdf(report_path)\n",
    "            report_clean = clean_text(report_text)\n",
    "            \n",
    "            # Handle chunking based on configuration\n",
    "            if config['enable_chunking']:\n",
    "                # Split texts into chunks\n",
    "                tenk_chunks = split_into_chunks(tenk_clean, config['chunk_size'])\n",
    "                report_chunks = split_into_chunks(report_clean, config['chunk_size'])\n",
    "                \n",
    "                # Pair each 10-K chunk with the corresponding report chunk\n",
    "                # Assuming one-to-one mapping; adjust if necessary\n",
    "                for i, (input_chunk, output_chunk) in enumerate(zip(tenk_chunks, report_chunks)):\n",
    "                    data_point = {\n",
    "                        'company': company,\n",
    "                        'input_text': f\"Company: {company}\\n[10-K Filing]\\n{input_chunk}\",\n",
    "                        'output_text': f\"Company: {company}\\n[Morningstar Report]\\n{output_chunk}\"\n",
    "                    }\n",
    "                    outfile.write(json.dumps(data_point) + '\\n')\n",
    "            else:\n",
    "                # Use the full texts without chunking\n",
    "                data_point = {\n",
    "                    'company': company,\n",
    "                    'input_text': f\"Company: {company}\\n[10-K Filing]\\n{tenk_clean}\",\n",
    "                    'output_text': f\"Company: {company}\\n[Morningstar Report]\\n{report_clean}\"\n",
    "                }\n",
    "                outfile.write(json.dumps(data_point) + '\\n')\n",
    "                    \n",
    "    print(f\"Data processing complete. Output saved to {config['output_file']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_dir': '../data',  # Root directory containing '10k' and 'reports' subdirectories\n",
    "    '10k_dir': '../data/10Ks',  # Directory containing 10-K PDFs\n",
    "    'reports_dir': '../data/reports',  # Directory containing Morningstar reports PDFs\n",
    "    'output_file': '../data/processed_data.jsonl',  # Output file path\n",
    "    'chunk_size': 1000,  # Characters per chunk if splitting is needed\n",
    "    'enable_chunking': False,\n",
    "    # 'companies': ['axp']  \n",
    "    'companies': ['axp', 'bac', 'cb', 'cvx', 'itochu',\n",
    "                  'khc', 'ko', 'mco', 'mitsubishi', 'oxy']  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Companies: 100%|██████████| 10/10 [05:47<00:00, 34.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output saved to ../data/processed_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
